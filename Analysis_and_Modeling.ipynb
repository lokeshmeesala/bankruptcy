{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "from warnings import filterwarnings\n",
    "filterwarnings(action=\"ignore\")\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import f1_score, classification_report, auc, roc_curve\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, PowerTransformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read The Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "one = pd.read_csv('Data/1year.csv')\n",
    "two = pd.read_csv('Data/2year.csv')\n",
    "three = pd.read_csv('Data/3year.csv')\n",
    "four = pd.read_csv('Data/4year.csv')\n",
    "five = pd.read_csv('Data/5year.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(one.shape)\n",
    "print(two.shape)\n",
    "print(three.shape)\n",
    "print(four.shape)\n",
    "print(five.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Function to calculate missing values by column\n",
    "def missing_values_table(df):\n",
    "        # Total missing values\n",
    "        mis_val = df.isnull().sum()\n",
    "        \n",
    "        # Percentage of missing values\n",
    "        mis_val_percent = 100 * df.isnull().sum() / len(df)\n",
    "        \n",
    "        # Make a table with the results\n",
    "        mis_val_table = pd.concat([mis_val, mis_val_percent], axis=1)\n",
    "        \n",
    "        # Add Dtype col\n",
    "        l = []\n",
    "        for i in mis_val_table.index:\n",
    "            l.append(df[i].dtype)\n",
    "        mis_val_table['dtype'] = l\n",
    "\n",
    "        # Rename the columns\n",
    "        mis_val_table_ren_columns = mis_val_table.rename(\n",
    "        columns = {0 : 'Missing Values', 1 : '% of Total Values'})\n",
    "        \n",
    "        # Sort the table by percentage of missing descending\n",
    "        mis_val_table_ren_columns = mis_val_table_ren_columns[\n",
    "            mis_val_table_ren_columns.iloc[:,1] != 0].sort_values(\n",
    "        '% of Total Values', ascending=False).round(1)\n",
    "        \n",
    "        # Print some summary information\n",
    "        print (\"Out of \" + str(df.shape[1]) + \" columns, \\n\"      \n",
    "            \" there are \" + str(mis_val_table_ren_columns.shape[0]) +\n",
    "              \" columns that have missing values.\")\n",
    "        \n",
    "        # Return the dataframe with missing information\n",
    "        return mis_val_table_ren_columns\n",
    "    \n",
    "\n",
    "def get_outliers_Zscore(data_series, threshold = 3, boxplot = False):\n",
    "    \n",
    "    # Calculate mean and standard deviation\n",
    "    m = data_series.mean()\n",
    "    s = data_series.std()\n",
    "    \n",
    "    # Calculate Z-scores and identify outliers based on the threshold\n",
    "    outliers_index =  data_series[data_series.apply(lambda x: abs((x - m)/s))>3].index.tolist()\n",
    "    return outliers_index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Missing Values\n",
    "- Missing values are entered as '?' and these features are read as object dtype, when infact they are float type.\n",
    "- They are replaced with NaN and changed the features as float type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "one.replace('?',np.NaN,inplace=True)\n",
    "two.replace('?',np.NaN,inplace=True)\n",
    "three.replace('?',np.NaN,inplace=True)\n",
    "four.replace('?',np.NaN,inplace=True)\n",
    "five.replace('?',np.NaN,inplace=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = list(one.columns)\n",
    "features.remove('class')\n",
    "\n",
    "one.loc[:,features] = one.loc[:,features].astype('float64')\n",
    "two.loc[:,features] = two.loc[:,features].astype('float64')\n",
    "three.loc[:,features] = three.loc[:,features].astype('float64')\n",
    "four.loc[:,features] = four.loc[:,features].astype('float64')\n",
    "five.loc[:,features] = five.loc[:,features].astype('float64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.concat([one,two,three,four,five],ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_values_table(data).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Outliers\n",
    "- It is observed that every column has outliers.\n",
    "- Ids of these outlier records are stored in outlier_id_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outlier_id_list = []\n",
    "for col in data.columns.values:\n",
    "    if (col != 'class') & (col != 'bankrupt_after_years'):\n",
    "        outlier_id_list.extend(get_outliers_Zscore(data[col]))\n",
    "outlier_id_list = set(outlier_id_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(6,2,figsize = (15,15))\n",
    "fig.suptitle('Comparision of distributions before and after removing outliers',fontsize=15)\n",
    "sns.kdeplot(data.Attr1.dropna(),color=\"r\",shade=True,ax=ax[0][0])\n",
    "sns.kdeplot(data.Attr1.drop(outlier_id_list).dropna(),color=\"g\",shade=True,ax=ax[0][1])\n",
    "\n",
    "sns.kdeplot(data.Attr2.dropna(),color=\"r\",shade=True,ax=ax[1][0])\n",
    "sns.kdeplot(data.Attr2.drop(outlier_id_list).dropna(),color=\"g\",shade=True,ax=ax[1][1])\n",
    "\n",
    "sns.kdeplot(data.Attr3.dropna(),color=\"r\",shade=True,ax=ax[2][0])\n",
    "sns.kdeplot(data.Attr3.drop(outlier_id_list).dropna(),color=\"g\",shade=True,ax=ax[2][1])\n",
    "\n",
    "sns.kdeplot(data.Attr4.dropna(),color=\"r\",shade=True,ax=ax[3][0])\n",
    "sns.kdeplot(data.Attr4.drop(outlier_id_list).dropna(),color=\"g\",shade=True,ax=ax[3][1])\n",
    "\n",
    "sns.kdeplot(data.Attr5.dropna(),color=\"r\",shade=True,ax=ax[4][0])\n",
    "sns.kdeplot(data.Attr5.drop(outlier_id_list).dropna(),color=\"g\",shade=True,ax=ax[4][1])\n",
    "\n",
    "sns.kdeplot(data.Attr6.dropna(),color=\"r\",shade=True,ax=ax[5][0])\n",
    "sns.kdeplot(data.Attr6.drop(outlier_id_list).dropna(),color=\"g\",shade=True,ax=ax[5][1])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Observations:\n",
    "- Distribution of values in each attributes are smoother after removing the outliers.\n",
    "- Applying transformations will futher normalize the distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1 = data.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Attr37 is having 43.7% of missing valuee. So removing it seems to be the best approach.\n",
    "data1.drop('Attr37',axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1.drop(outlier_id_list,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1.reset_index(drop=True,inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Missing values\n",
    "- Missing values are filled with median of the respective attributes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in data1.columns.values:\n",
    "    data1[col].fillna(data1[col].median(),inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scaling\n",
    "- 3 Different types of scaling methods are used.\n",
    "    - MinMaxScaler - Uses minimum and maximum of the series.\n",
    "    - StandardScaler - Uses mean and zscore of the series.\n",
    "    - PowerTransformer - Parametric and Monotonic transformations, useful when heteroscedasticity is present."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mms = MinMaxScaler()\n",
    "sscaler = StandardScaler()\n",
    "ptransform = PowerTransformer(method='yeo-johnson')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PowerTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in data1.columns.values:\n",
    "    if col != 'class': \n",
    "        data1[col] = ptransform.fit_transform(data1[col].values.reshape(-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(6,2,figsize = (12,10))\n",
    "fig.suptitle('Impact of power transfomration on the data distribution',fontsize=15)\n",
    "sns.kdeplot(data.Attr1.fillna(data.Attr1.median()),color=\"r\",shade=True,ax=ax[0][0])\n",
    "sns.kdeplot(data1.Attr1,color=\"g\",shade=True,ax=ax[0][1])\n",
    "\n",
    "sns.kdeplot(data.Attr2.fillna(data.Attr2.median()),color=\"r\",shade=True,ax=ax[1][0])\n",
    "sns.kdeplot(data1.Attr2,color=\"g\",shade=True,ax=ax[1][1])\n",
    "\n",
    "sns.kdeplot(data.Attr3.fillna(data.Attr3.median()),color=\"r\",shade=True,ax=ax[2][0])\n",
    "sns.kdeplot(data1.Attr3,color=\"g\",shade=True,ax=ax[2][1])\n",
    "\n",
    "sns.kdeplot(data.Attr4.fillna(data.Attr4.median()),color=\"r\",shade=True,ax=ax[3][0])\n",
    "sns.kdeplot(data1.Attr4,color=\"g\",shade=True,ax=ax[3][1])\n",
    "\n",
    "sns.kdeplot(data.Attr5.fillna(data.Attr5.median()),color=\"r\",shade=True,ax=ax[4][0])\n",
    "sns.kdeplot(data1.Attr5,color=\"g\",shade=True,ax=ax[4][1])\n",
    "\n",
    "sns.kdeplot(data.Attr6.fillna(data.Attr6.median()),color=\"r\",shade=True,ax=ax[5][0])\n",
    "sns.kdeplot(data1.Attr6,color=\"g\",shade=True,ax=ax[5][1])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling\n",
    "- model_df is used to collect results from all the experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_df = pd.DataFrame(columns=['Model','Description','Train_F1','Test_F1','AUC'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data1.drop(['class'],axis=1)\n",
    "y = data1['class']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y, train_size = 0.7,random_state=123)\n",
    "\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_train.shape) \n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGB Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_model = XGBClassifier(n_estimators=250,max_depth=5)\n",
    "xgb_model.fit(X_train,y_train)\n",
    "\n",
    "y_train_preds = xgb_model.predict(X_train)\n",
    "y_test_preds = xgb_model.predict(X_test)\n",
    "\n",
    "fpr, tpr, thresholds = roc_curve(y_test,y_test_preds)\n",
    "auc_score = auc(fpr,tpr)\n",
    "\n",
    "df1 = {'Model':'XGBoost_Power_Transform',\n",
    "       'Description':'n_estimators=250,max_depth=5, na imputed with median',\n",
    "       'Train_F1':f1_score(y_train,y_train_preds),\n",
    "       'Test_F1':f1_score(y_test,y_test_preds),\n",
    "       'AUC':auc_score}\n",
    "model_df = pd.concat([model_df, pd.DataFrame([df1])], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_model = XGBClassifier(n_estimators=300,max_depth=5)\n",
    "xgb_model.fit(X_train,y_train)\n",
    "\n",
    "y_train_preds = xgb_model.predict(X_train)\n",
    "y_test_preds = xgb_model.predict(X_test)\n",
    "\n",
    "fpr, tpr, thresholds = roc_curve(y_test,y_test_preds)\n",
    "auc_score = auc(fpr,tpr)\n",
    "\n",
    "df1 = {'Model':'XGBoost_Power_Transform',\n",
    "       'Description':'n_estimators=300,max_depth=5, na imputed with median',\n",
    "       'Train_F1':f1_score(y_train,y_train_preds),\n",
    "       'Test_F1':f1_score(y_test,y_test_preds),\n",
    "       'AUC':auc_score}\n",
    "model_df = pd.concat([model_df, pd.DataFrame([df1])], ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = { 'n_estimators': [200,300,400,500],\n",
    "          'max_depth': [5,6,7],\n",
    "          'learning_rate' : [0.1, 0.01, 0.05, 0.3]\n",
    "         }\n",
    "\n",
    "grid_xgb = GridSearchCV(estimator= XGBClassifier(), param_grid = params, cv=5)\n",
    "\n",
    "grid_xgb.fit(X_train,y_train)\n",
    "\n",
    "\n",
    "y_train_preds = grid_xgb.predict(X_train)\n",
    "y_test_preds = grid_xgb.predict(X_test)\n",
    "\n",
    "print('Train F1 Score:', f1_score(y_train,y_train_preds))\n",
    "print('Test F1 Score:', f1_score(y_test,y_test_preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGB models with best params from Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_model = XGBClassifier(n_estimators=250,max_depth=4,learning_rate=0.3,gamma=0.1)\n",
    "xgb_model.fit(X_train,y_train)\n",
    "\n",
    "y_train_preds = xgb_model.predict(X_train)\n",
    "y_test_preds = xgb_model.predict(X_test)\n",
    "\n",
    "fpr, tpr, thresholds = roc_curve(y_test,y_test_preds)\n",
    "auc_score = auc(fpr,tpr)\n",
    "\n",
    "df1 = {'Model':'XGBoost_Power_Transform_Grid_serach',\n",
    "        'Description':'n_estimators=250,max_depth=4,learning_rate=0.3,gamma=0.1, na imputed with median',\n",
    "       'Train_F1':f1_score(y_train,y_train_preds),\n",
    "       'Test_F1':f1_score(y_test,y_test_preds),\n",
    "       'AUC':auc_score}\n",
    "model_df = pd.concat([model_df, pd.DataFrame([df1])], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_model = XGBClassifier(n_estimators=250,max_depth=4,learning_rate=0.3,gamma=0.5)\n",
    "xgb_model.fit(X_train,y_train)\n",
    "\n",
    "y_train_preds = xgb_model.predict(X_train)\n",
    "y_test_preds = xgb_model.predict(X_test)\n",
    "\n",
    "fpr, tpr, thresholds = roc_curve(y_test,y_test_preds)\n",
    "auc_score = auc(fpr,tpr)\n",
    "\n",
    "df1 = {'Model':'XGBoost_Power_Transform_Grid_serach',\n",
    "        'Description':'n_estimators=250,max_depth=4,learning_rate=0.3,gamma=0.5, na imputed with median',\n",
    "       'Train_F1':f1_score(y_train,y_train_preds),\n",
    "       'Test_F1':f1_score(y_test,y_test_preds),\n",
    "       'AUC':auc_score}\n",
    "model_df = pd.concat([model_df, pd.DataFrame([df1])], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Minmax Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mms = MinMaxScaler()\n",
    "\n",
    "for col in data.columns.values:\n",
    "    data[col].fillna(data[col].median(),inplace=True)\n",
    "\n",
    "for col in data.columns.values:\n",
    "    if col != 'class': \n",
    "        data[col] = mms.fit_transform(data[col].values.reshape(-1,1))\n",
    "\n",
    "X = data.drop(['class'],axis=1)\n",
    "y = data['class']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y, train_size = 0.7,random_state=123)\n",
    "\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_train.shape) \n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_model = XGBClassifier(n_estimators=250,max_depth=5,learning_rate=0.3, gamma=0.5)\n",
    "xgb_model.fit(X_train,y_train)\n",
    "\n",
    "y_train_preds = xgb_model.predict(X_train)\n",
    "y_test_preds = xgb_model.predict(X_test)\n",
    "\n",
    "fpr, tpr, thresholds = roc_curve(y_test,y_test_preds)\n",
    "auc_score = auc(fpr,tpr)\n",
    "\n",
    "df1 = {'Model':'XGBoost_MinMax_Scaler',\n",
    "        'Description':'n_estimators=250,max_depth=5,learning_rate=0.3,gamma=0.5, na imputed with median',\n",
    "       'Train_F1':f1_score(y_train,y_train_preds),\n",
    "       'Test_F1':f1_score(y_test,y_test_preds),\n",
    "       'AUC':auc_score}\n",
    "model_df = pd.concat([model_df, pd.DataFrame([df1])], ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variance Inflation Factor\n",
    "- Dropping highly correlated features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.stats.outliers_influence import variance_inflation_factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_vif(X, thresh=5.0):\n",
    "        # Taken from https://stats.stackexchange.com/a/253620/53565 and modified\n",
    "        dropped=True\n",
    "        while dropped:\n",
    "            variables = X.columns\n",
    "            dropped = False\n",
    "            vif = [variance_inflation_factor(X[variables].values, X.columns.get_loc(var)) for var in X.columns]\n",
    "            \n",
    "            max_vif = max(vif)\n",
    "            if max_vif > thresh:\n",
    "                maxloc = vif.index(max_vif)\n",
    "                print(f'Dropping {X.columns[maxloc]} with vif={max_vif}')\n",
    "                X = X.drop([X.columns.tolist()[maxloc]], axis=1)\n",
    "                dropped=True\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data1.drop(['class'],axis=1)\n",
    "y = data1['class']\n",
    "\n",
    "X_vif = calculate_vif(X,thresh= 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modelling after dropping correlated features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_vif,y, train_size = 0.7,random_state=123)\n",
    "\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_train.shape) \n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_model = XGBClassifier()\n",
    "xgb_model.fit(X_train,y_train)\n",
    "\n",
    "y_train_preds = xgb_model.predict(X_train)\n",
    "y_test_preds = xgb_model.predict(X_test)\n",
    "\n",
    "fpr, tpr, thresholds = roc_curve(y_test,y_test_preds)\n",
    "auc_score = auc(fpr,tpr)\n",
    "\n",
    "df1 = {'Model':'XGBoost_VIF_Power_Transform',\n",
    "       'Description':'Base, na imputed with median',\n",
    "       'Train_F1':f1_score(y_train,y_train_preds),\n",
    "       'Test_F1':f1_score(y_test,y_test_preds),\n",
    "       'AUC':auc_score}\n",
    "model_df = pd.concat([model_df, pd.DataFrame([df1])], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_model = XGBClassifier(max_depth=5)\n",
    "xgb_model = XGBClassifier()\n",
    "xgb_model.fit(X_train,y_train)\n",
    "\n",
    "y_train_preds = xgb_model.predict(X_train)\n",
    "y_test_preds = xgb_model.predict(X_test)\n",
    "\n",
    "fpr, tpr, thresholds = roc_curve(y_test,y_test_preds)\n",
    "auc_score = auc(fpr,tpr)\n",
    "\n",
    "df1 = {'Model':'XGBoost_VIF_Power_Transform',\n",
    "       'Description':'max_depth = 5, na imputed with median',\n",
    "       'Train_F1':f1_score(y_train,y_train_preds),\n",
    "       'Test_F1':f1_score(y_test,y_test_preds),\n",
    "       'AUC':auc_score}\n",
    "model_df = pd.concat([model_df, pd.DataFrame([df1])], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
